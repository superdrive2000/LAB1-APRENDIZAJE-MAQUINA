[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA  \n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    shape=\"Origin\",\n    color = \"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\nGrafico de barras\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='y'),\n    alt.Y('count()')\n)\n\n\n\n\n\n\n\n\nMean()de cada uno de los origenes\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width= 200,\n    height = 200\n)\n\n\n\n\n\n\n\n\nGrafico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width = 600\n).interactive()"
  },
  {
    "objectID": "index.html#importar-el-dataset",
    "href": "index.html#importar-el-dataset",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Code\nimport altair as alt\nfrom vega_datasets import data\ncars = data.cars()\nprint(cars.head())\n\n\n                        Name  Miles_per_Gallon  Cylinders  Displacement  \\\n0  chevrolet chevelle malibu              18.0          8         307.0   \n1          buick skylark 320              15.0          8         350.0   \n2         plymouth satellite              18.0          8         318.0   \n3              amc rebel sst              16.0          8         304.0   \n4                ford torino              17.0          8         302.0   \n\n   Horsepower  Weight_in_lbs  Acceleration       Year Origin  \n0       130.0           3504          12.0 1970-01-01    USA  \n1       165.0           3693          11.5 1970-01-01    USA  \n2       150.0           3436          11.0 1970-01-01    USA  \n3       150.0           3433          12.0 1970-01-01    USA  \n4       140.0           3449          10.5 1970-01-01    USA"
  },
  {
    "objectID": "index.html#visualizaci√≥n-en-altair",
    "href": "index.html#visualizaci√≥n-en-altair",
    "title": "PRIMER REPORTE EN QUARTO",
    "section": "",
    "text": "Code\nalt.Chart(cars).mark_point().encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n#interactive: zoom, brush\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    color=\"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\nCode\nalt.Chart(cars).mark_point(filled=True).encode(\n    x=\"Miles_per_Gallon\",\n    y=\"Horsepower\",\n    shape=\"Origin\",\n    color = \"Origin\",\n    tooltip=[\"Name\",\"Origin\",\"Miles_per_Gallon\",\"Horsepower\"]\n).interactive()\n\n\n\n\n\n\n\n\n\n\n\nGrafico de barras\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='y'),\n    alt.Y('count()')\n)\n\n\n\n\n\n\n\n\nMean()de cada uno de los origenes\n\n\nCode\nalt.Chart(cars).mark_bar().encode(\n    alt.X('Origin',sort='y'),\n    alt.Y('mean(Weight_in_lbs)'),\n    tooltip=['Origin','mean(Weight_in_lbs)']\n).properties(\n    width= 200,\n    height = 200\n)\n\n\n\n\n\n\n\n\nGrafico lineal\n\n\nCode\nalt.Chart(cars).mark_line(point=True).encode(\n    alt.X('Year'),\n    alt.Y('mean(Weight_in_lbs)'),\n    alt.Color('Origin')\n).properties(\n    width = 600\n).interactive()"
  },
  {
    "objectID": "lab.html",
    "href": "lab.html",
    "title": "LAB1. PERSONAS DESAPARECIDAS EN ECUADOR",
    "section": "",
    "text": "Code\nimport altair as alt\nimport pandas as pd\npersonas = pd.read_excel('mdi_personas_desaparecidas_pm_2025_enero_a_agosto.xlsx',sheet_name='1')\nprint(personas.head())\n\n\n     zona   provincia      canton    distrito        circuito  \\\n0  ZONA 3  CHIMBORAZO     GUAMOTE       COLTA         PALMIRA   \n1  ZONA 3  CHIMBORAZO    RIOBAMBA    RIOBAMBA      24 DE MAYO   \n2  ZONA 3  CHIMBORAZO      CHAMBO    RIOBAMBA          CHAMBO   \n3  ZONA 4      MANAB√ç       MANTA       MANTA      MIRAFLORES   \n4  ZONA 4      MANAB√ç  PEDERNALES  PEDERNALES  PEDERNALES SUR   \n\n        subcircuito    sexo nacionalidad  edad_aproximada    rango_edad  \\\n0         PALMIRA 1   MUJER      ECUADOR               14  ADOLESCENTES   \n1      24 DE MAYO 1   MUJER      ECUADOR               15  ADOLESCENTES   \n2          CHAMBO 1   MUJER      ECUADOR               15  ADOLESCENTES   \n3      MIRAFLORES 2  HOMBRE      ECUADOR               49        ADULTO   \n4  PEDERNALES SUR 1   MUJER      ECUADOR               14  ADOLESCENTES   \n\n       etnia   fecha_localizacion motivo_desaparicion  \\\n0   INDIGENA             SIN_DATO            SIN_DATO   \n1  MESTIZO/A  2025-09-03 00:00:00   CAUSAS PERSONALES   \n2   INDIGENA  2025-09-12 00:00:00   CAUSAS PERSONALES   \n3  MESTIZO/A             SIN_DATO            SIN_DATO   \n4  MESTIZO/A  2025-09-04 00:00:00   CAUSAS FAMILIARES   \n\n  motivacion_observacion_observada situacion_actual    latitud_desaparicion  \\\n0                         SIN_DATO     DESAPARECIDO   '-2,06151971173764226   \n1                RELACI√ìN AFECTIVA       ENCONTRADO   '-1,66017374932846629   \n2                RELACI√ìN AFECTIVA       ENCONTRADO   '-1,73642999999999992   \n3                         SIN_DATO     DESAPARECIDO  '-0,957013777773502827   \n4      NO RELACIONADOS A VIOLENCIA       ENCONTRADO  '0,0810089999999999977   \n\n   longitud_desaparicion fecha_denuncia fecha_desaparicion  \n0  '-78,7373445443988942     2025-09-10         2025-08-31  \n1  '-78,7023779013353817     2025-09-01         2025-08-31  \n2  '-78,6013250000000028     2025-09-01         2025-08-31  \n3   '-80,723711371512664     2025-09-04         2025-08-31  \n4  '-80,0544900000000013     2025-09-02         2025-08-31"
  },
  {
    "objectID": "lab.html#personas-desaparecidas-enero-a-agosto-2025",
    "href": "lab.html#personas-desaparecidas-enero-a-agosto-2025",
    "title": "LAB1. PERSONAS DESAPARECIDAS EN ECUADOR",
    "section": "",
    "text": "Code\nimport altair as alt\nimport pandas as pd\npersonas = pd.read_excel('mdi_personas_desaparecidas_pm_2025_enero_a_agosto.xlsx',sheet_name='1')\nprint(personas.head())\n\n\n     zona   provincia      canton    distrito        circuito  \\\n0  ZONA 3  CHIMBORAZO     GUAMOTE       COLTA         PALMIRA   \n1  ZONA 3  CHIMBORAZO    RIOBAMBA    RIOBAMBA      24 DE MAYO   \n2  ZONA 3  CHIMBORAZO      CHAMBO    RIOBAMBA          CHAMBO   \n3  ZONA 4      MANAB√ç       MANTA       MANTA      MIRAFLORES   \n4  ZONA 4      MANAB√ç  PEDERNALES  PEDERNALES  PEDERNALES SUR   \n\n        subcircuito    sexo nacionalidad  edad_aproximada    rango_edad  \\\n0         PALMIRA 1   MUJER      ECUADOR               14  ADOLESCENTES   \n1      24 DE MAYO 1   MUJER      ECUADOR               15  ADOLESCENTES   \n2          CHAMBO 1   MUJER      ECUADOR               15  ADOLESCENTES   \n3      MIRAFLORES 2  HOMBRE      ECUADOR               49        ADULTO   \n4  PEDERNALES SUR 1   MUJER      ECUADOR               14  ADOLESCENTES   \n\n       etnia   fecha_localizacion motivo_desaparicion  \\\n0   INDIGENA             SIN_DATO            SIN_DATO   \n1  MESTIZO/A  2025-09-03 00:00:00   CAUSAS PERSONALES   \n2   INDIGENA  2025-09-12 00:00:00   CAUSAS PERSONALES   \n3  MESTIZO/A             SIN_DATO            SIN_DATO   \n4  MESTIZO/A  2025-09-04 00:00:00   CAUSAS FAMILIARES   \n\n  motivacion_observacion_observada situacion_actual    latitud_desaparicion  \\\n0                         SIN_DATO     DESAPARECIDO   '-2,06151971173764226   \n1                RELACI√ìN AFECTIVA       ENCONTRADO   '-1,66017374932846629   \n2                RELACI√ìN AFECTIVA       ENCONTRADO   '-1,73642999999999992   \n3                         SIN_DATO     DESAPARECIDO  '-0,957013777773502827   \n4      NO RELACIONADOS A VIOLENCIA       ENCONTRADO  '0,0810089999999999977   \n\n   longitud_desaparicion fecha_denuncia fecha_desaparicion  \n0  '-78,7373445443988942     2025-09-10         2025-08-31  \n1  '-78,7023779013353817     2025-09-01         2025-08-31  \n2  '-78,6013250000000028     2025-09-01         2025-08-31  \n3   '-80,723711371512664     2025-09-04         2025-08-31  \n4  '-80,0544900000000013     2025-09-02         2025-08-31"
  },
  {
    "objectID": "lab.html#visualizaci√≥n-en-altair",
    "href": "lab.html#visualizaci√≥n-en-altair",
    "title": "LAB1. PERSONAS DESAPARECIDAS EN ECUADOR",
    "section": "Visualizaci√≥n en Altair",
    "text": "Visualizaci√≥n en Altair\n\nVisaulizacion 1: Pie Graph Distribuci√≥n por rango de edad\n\n\nCode\nedades = personas[\"rango_edad\"].value_counts().reset_index()\nedades.columns = [\"rango_edad\", \"cantidad\"]\n\nalt.Chart(edades).mark_arc().encode(\n    theta=alt.Theta(field=\"cantidad\", type=\"quantitative\"),\n    color=alt.Color(field=\"rango_edad\", type=\"nominal\"),\n    tooltip=[\"rango_edad\", \"cantidad\"]\n).properties(\n    title=\"Distribuci√≥n de desaparecidos por rango de edad\",\n    width=400, height=400\n)\n\n#interactive: zoom, brush\n\n\n\n\n\n\n\n\n\n\nVisaulizacion 2:\n\n\nCode\ndata_motivos = personas[[\"motivo_desaparicion\"]].copy()\nmotivos_top = data_motivos[\"motivo_desaparicion\"].value_counts().nlargest(10).index.tolist()\ndata_motivos = data_motivos[data_motivos[\"motivo_desaparicion\"].isin(motivos_top)]\n\nalt.Chart(data_motivos).mark_bar().encode(\n    y=alt.Y(\"motivo_desaparicion:N\", sort='-x', title=\"Motivo de desaparici√≥n\"),\n    x=alt.X(\"count()\", title=\"Cantidad\"),\n    color=\"motivo_desaparicion:N\",\n    tooltip=[\"motivo_desaparicion\", \"count()\"]\n).properties(\n    title=\"Motivos m√°s frecuentes de desaparici√≥n (Top 10)\",\n    width=500, height=300\n)\n\n\n\n\n\n\n\n\n\n\nVisaulizacion 3:\n\n\nCode\ndata = personas[['sexo']]\n\nalt.Chart(data).mark_bar().encode(\n    x=alt.X(\"sexo:N\", title=\"Sexo\"),\n    y=alt.Y(\"count()\", title=\"Cantidad\"),\n    color=\"sexo:N\",\n    tooltip=[\"sexo\", \"count()\"]\n).properties(\n    title=\"Distribuci√≥n de personas desaparecidas por sexo\",\n    width=400, height=300\n)"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is one of the fundamental steps in any data science process. It allows us to understand the structure, detect anomalies, and uncover patterns in the data before modeling.\n\n‚ÄúWithout EDA, you‚Äôre not doing data science, you‚Äôre just guessing.‚Äù\n\nEDA combines statistics, programming, and visualization to explore datasets. This report is designed to help you practice these core skills using real-world data.\n\n\nWe will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet‚Äôs load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let‚Äôs examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset.\n\n\n\nBefore diving deeper into the data, it‚Äôs useful to explore some key metadata:\n\n‚úÖ The column names and their data types\n‚ö†Ô∏è The presence of missing values\nüìä Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we‚Äôre dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB\n\n\n\n\n\n\nDetecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet‚Äôs start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows √ó 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let‚Äôs set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis.\n\n\n\n\nFinally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows √ó 15 columns"
  },
  {
    "objectID": "eda.html#dataset",
    "href": "eda.html#dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "We will use the movies dataset from vega-datasets, which includes information about thousands of films such as their ratings, genres, duration, and box office revenue.\nLet‚Äôs load and preview the dataset:\n\n\nCode\nimport pandas as pd\nimport altair as alt\nfrom vega_datasets import data\n\n# Load dataset\nmovies = data.movies()\n\n# Show first rows\nmovies.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\nNow, let‚Äôs examine the shape (number of rows and columns) of the dataset:\n\n\nCode\nmovies.shape\n\n\n(3201, 16)\n\n\nThis tells us how many entries (rows) and features (columns) are present in the dataset."
  },
  {
    "objectID": "eda.html#first-steps",
    "href": "eda.html#first-steps",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Before diving deeper into the data, it‚Äôs useful to explore some key metadata:\n\n‚úÖ The column names and their data types\n‚ö†Ô∏è The presence of missing values\nüìä Summary statistics for numeric columns\n\n\n\nUnderstanding the structure of the dataset helps us know what type of data we‚Äôre dealing with.\n\n\nCode\nmovies.dtypes\n\n\nTitle                      object\nUS_Gross                  float64\nWorldwide_Gross           float64\nUS_DVD_Sales              float64\nProduction_Budget         float64\nRelease_Date               object\nMPAA_Rating                object\nRunning_Time_min          float64\nDistributor                object\nSource                     object\nMajor_Genre                object\nCreative_Type              object\nDirector                   object\nRotten_Tomatoes_Rating    float64\nIMDB_Rating               float64\nIMDB_Votes                float64\ndtype: object\n\n\nWe can also use .info() for a more complete summary, including non-null counts:\n\n\nCode\n# Overview of the dataset\nmovies.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3201 entries, 0 to 3200\nData columns (total 16 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   Title                   3200 non-null   object \n 1   US_Gross                3194 non-null   float64\n 2   Worldwide_Gross         3194 non-null   float64\n 3   US_DVD_Sales            564 non-null    float64\n 4   Production_Budget       3200 non-null   float64\n 5   Release_Date            3201 non-null   object \n 6   MPAA_Rating             2596 non-null   object \n 7   Running_Time_min        1209 non-null   float64\n 8   Distributor             2969 non-null   object \n 9   Source                  2836 non-null   object \n 10  Major_Genre             2926 non-null   object \n 11  Creative_Type           2755 non-null   object \n 12  Director                1870 non-null   object \n 13  Rotten_Tomatoes_Rating  2321 non-null   float64\n 14  IMDB_Rating             2988 non-null   float64\n 15  IMDB_Votes              2988 non-null   float64\ndtypes: float64(8), object(8)\nmemory usage: 400.3+ KB"
  },
  {
    "objectID": "eda.html#missing-values",
    "href": "eda.html#missing-values",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Detecting and handling missing values is a critical step in any EDA process. Missing data can bias analysis or break downstream models if not handled properly.\n\nDetect patterns in missingness\nIdentify if some columns are almost entirely null\nDecide whether to drop or impute certain variables\n\n\n\nLet‚Äôs start by computing the percentage of missing values in each column:\n\n\nCode\nnan_percent = movies.isna().mean() * 100\nnan_percent_sorted = nan_percent.sort_values(ascending=False).round(2)\nnan_percent_sorted\n\n\nUS_DVD_Sales              82.38\nRunning_Time_min          62.23\nDirector                  41.58\nRotten_Tomatoes_Rating    27.49\nMPAA_Rating               18.90\nCreative_Type             13.93\nSource                    11.40\nMajor_Genre                8.59\nDistributor                7.25\nIMDB_Rating                6.65\nIMDB_Votes                 6.65\nUS_Gross                   0.22\nWorldwide_Gross            0.22\nTitle                      0.03\nProduction_Budget          0.03\nRelease_Date               0.00\ndtype: float64\n\n\n\n\n\nTo visualize missing values with Altair, we need to reshape the data into a long format where each missing value is a row:\n\n\nCode\nmovies_nans = movies.isna().reset_index().melt(\n    id_vars='index',\n    var_name='column',\n    value_name=\"NaN\"\n)\nmovies_nans\n\n\n\n\n\n\n\n\n\nindex\ncolumn\nNaN\n\n\n\n\n0\n0\nTitle\nFalse\n\n\n1\n1\nTitle\nFalse\n\n\n2\n2\nTitle\nFalse\n\n\n3\n3\nTitle\nFalse\n\n\n4\n4\nTitle\nFalse\n\n\n...\n...\n...\n...\n\n\n51211\n3196\nIMDB_Votes\nFalse\n\n\n51212\n3197\nIMDB_Votes\nTrue\n\n\n51213\n3198\nIMDB_Votes\nFalse\n\n\n51214\n3199\nIMDB_Votes\nFalse\n\n\n51215\n3200\nIMDB_Votes\nFalse\n\n\n\n\n51216 rows √ó 3 columns\n\n\n\n\n\n\nThis heatmap shows where missing values occur across rows and columns. Patterns may indicate:\n\nColumns with consistently missing values\nEntire rows with large gaps\nCorrelated missingness between variables\n\nTo avoid limitations in the number of rows rendered by Altair, we disable the max rows warning:\n\n\nCode\nalt.data_transformers.disable_max_rows()\n\n\nDataTransformerRegistry.enable('default')\n\n\nNow we can create the heatmap:\n\n\nCode\nalt.Chart(movies_nans).mark_rect().encode(\n    alt.X('index:O'),\n    alt.Y('column'),\n    alt.Color('NaN')\n).properties(\n    width=1000\n)\n\n\n\n\n\n\n\n\nThis plot can help identify columns or rows with critical data issues.\n\n\n\nIn many real-world cases, we may decide to remove columns that have too many missing values. Let‚Äôs set a threshold of 70%:\n\n\nCode\nthreshold_nan = 70 # in percent\ncols_to_drop = nan_percent[nan_percent&gt;threshold_nan].index\ncols_to_drop\n\n\nIndex(['US_DVD_Sales'], dtype='object')\n\n\nThese columns have more than 70% missing values and may not be useful for analysis."
  },
  {
    "objectID": "eda.html#cleaned-dataset",
    "href": "eda.html#cleaned-dataset",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Finally, we drop the selected columns and inspect the updated dataset:\n\n\nCode\nmovies_cleaned = movies.drop(columns=cols_to_drop)\nmovies_cleaned\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3196\nZack and Miri Make a Porno\n31452765.0\n36851125.0\n24000000.0\nOct 31 2008\nR\n101.0\nWeinstein Co.\nOriginal Screenplay\nComedy\nContemporary Fiction\nKevin Smith\n65.0\n7.0\n55687.0\n\n\n3197\nZodiac\n33080084.0\n83080084.0\n85000000.0\nMar 02 2007\nR\n157.0\nParamount Pictures\nBased on Book/Short Story\nThriller/Suspense\nDramatization\nDavid Fincher\n89.0\nNaN\nNaN\n\n\n3198\nZoom\n11989328.0\n12506188.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\n\n\n\n\n3201 rows √ó 15 columns"
  },
  {
    "objectID": "eda.html#univariate-analysis-quantitative",
    "href": "eda.html#univariate-analysis-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.1 Univariate Analysis: Quantitative",
    "text": "2.1 Univariate Analysis: Quantitative\nA univariate analysis focuses on examining a single numeric variable to understand its distribution, shape, central tendency, and spread. One of the most common tools for this is the histogram.\nIn this case, we‚Äôll explore the distribution of the movie runtime (Running_Time_min).\n\n2.1.1 Basic Histogram\nWe start by creating a histogram to visualize the distribution of running times:\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    title='Histogram of Movie Runtimes (30 bins)'\n)\n\n\n\n\n\n\n\n\nThis chart shows how many movies fall into each time interval (bin). However, histograms can look quite different depending on the number and size of bins used.\n\n\n2.1.2 Effect of Bin Size\nLet‚Äôs compare how the histogram shape changes with different bin sizes:\n\n\nCode\nhistogram_1 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=8)),\n    alt.Y('count()')\n)\n\nhistogram_2 = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('Running_Time_min',bin=alt.Bin(maxbins=10)),\n    alt.Y('count()')\n)\n\nhistogram_1 | histogram_2\n\n\n\n\n\n\n\n\nEven though both plots use the same data, the choice of bin size changes the visual interpretation. A small number of bins may hide details, while too many bins can make it harder to spot trends.\n\n\n2.1.3 Density plots, or Kernel Density Estimate (KDE)\nDensity plots offer a smoothed alternative to histograms. Instead of using rectangular bins to count data points, they estimate the probability density function by placing bell-shaped curves (kernels) at each observation and summing them.\nThis approach helps reduce the visual noise and jaggedness that can occur in histograms and gives a clearer picture of the underlying distribution.\n\n\nCode\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    as_=['Running_Time_min','density'],\n).mark_area().encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q')\n).properties(\n    title=\"Movies runtime\"\n)\n\n\n\n\n\n\n\n\n\n\n2.1.4 Grouped Density plot\nWe can also compare distributions across groups by splitting the KDE by a categorical variable using the groupby parameter. This helps us see how the distribution differs between categories, such as genres.\n\n\nCode\nselection = alt.selection_point(fields=['Major_Genre'], bind='legend')\n\nalt.Chart(movies_cleaned).transform_density(\n    'Running_Time_min',\n    groupby=['Major_Genre'],\n    as_=['Running_Time_min', 'density'],\n).mark_area(opacity=0.5).encode(\n    alt.X('Running_Time_min'),\n    alt.Y('density:Q', stack=None),\n    alt.Color('Major_Genre'),\n    opacity=alt.condition(selection, \n        alt.value(1), \n        alt.value(0.05)\n    )\n).add_params(\n    selection\n).properties(\n    title=\"Movies Runtime by Genre (Interactive Filter)\"\n).interactive()\n\n\n\n\n\n\n\n\nThe transparency (opacity=0.5) allows us to observe overlapping distributions and ensures that small density areas are not completely hidden behind larger ones.\nFrom this plot, we can observe, for example, that Drama movies have runtimes nearly as long as the longest Adventure movies, even though their overall distributions differ."
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "href": "eda.html#bivariate-analysis-categorical-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.2 Bivariate Analysis: Categorical vs Quantitative",
    "text": "2.2 Bivariate Analysis: Categorical vs Quantitative\nBivariate analysis examines the relationship between two variables. In this case, we focus on one categorical variable (e.g., genre) and one quantitative variable (e.g., revenue), which is a very common scenario in exploratory data analysis.\nThis type of analysis is useful to: - Compare average or median values across categories. - Detect outliers or high-variance groups. - Understand distributional differences across categories.\nBelow are several effective visualizations for this analysis.\n\n2.2.1 Basic Barchart\nBar charts are effective for comparing aggregated values (like the mean) across different groups. However, they hide the distribution and variation within each group.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Average Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\nThis bar chart shows the mean Worldwide Gross per genre. It is useful for identifying which genres are more profitable on average, but does not show how spread out the data is.\n\n\n2.2.2 Tick Plot\nTo visualize individual data points, we use a tick plot. This helps uncover variability within genres and detect outliers.\n\n\nCode\nalt.Chart(movies_cleaned).mark_tick().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\"),\n    alt.Tooltip('Title:N')\n).properties(\n    title=\"Individual Gross per Movie by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.3 Heatmaps\nHeatmaps can summarize the frequency of data points across both axes (quantitative and categorical) using color intensity. It‚Äôs particularly useful for spotting patterns without getting overwhelmed by individual points.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Worldwide_Gross',bin=alt.Bin(maxbins=100)),\n    alt.Y(\"Major_Genre\"),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Heatmap of Movie Counts by Gross and Genre\"\n)\n\n\n\n\n\n\n\n\nThis heatmap shows how frequently movies from each genre fall into different revenue ranges.\n\n\n2.2.4 Boxplot\nBoxplots are useful for comparing distributions across categories and identifying outliers. Boxplots summarize a distribution using five statistics:\n\nMedian (Q2)\nFirst Quartile (Q1)\nThird Quartile (Q3)\nLower Whisker (Q1 - 1.5 √ó IQR)\nUpper Whisker (Q3 + 1.5 √ó IQR)\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Worldwide_Gross'),\n    alt.Y(\"Major_Genre\")\n).properties(\n    title=\"Boxplot of Worldwide Gross by Genre\"\n)\n\n\n\n\n\n\n\n\n\n\n2.2.5 Side-by-side: Boxplot and Bar Chart\nTo contrast aggregated values (bar chart) with the full distribution (boxplot), we can display them together:\n\n\nCode\nbar = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox = alt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('mean(Worldwide_Gross)'),\n    alt.Y(\"Major_Genre\")\n)\n\nbox | bar\n\n\n\n\n\n\n\n\nThis comparison reveals whether the mean is a good representative of the genre, or whether the data is skewed or contains outliers that affect the average"
  },
  {
    "objectID": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "href": "eda.html#bivariate-analysis-quantitative-vs-quantitative",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.3 Bivariate Analysis: Quantitative vs Quantitative",
    "text": "2.3 Bivariate Analysis: Quantitative vs Quantitative\nWhen analyzing two quantitative (numerical) variables simultaneously, we aim to discover possible relationships, trends, or correlations. This type of bivariate analysis can reveal whether increases in one variable are associated with increases or decreases in another (positive or negative correlation), or if there‚Äôs no relationship at all. The most common and intuitive visualization for this is the scatterplot.\n\n2.3.1 Scatterplots\nScatter plots are effective visualizations for exploring two-dimensional distributions, allowing us to identify patterns, trends, clusters, or outliers.\nLet‚Äôs start by visualizing how movies are rated across two popular online platforms:\n\nIMDb\n\nRotten Tomatoes\n\nAre movies rated similarly on different platforms?\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('IMDB_Rating'),\n    alt.Y('Rotten_Tomatoes_Rating')\n).properties(\n    title=\"IMDB vs Rotten Tomatoes Ratings\"\n)\n\n\n\n\n\n\n\n\n\n\n2.3.2 Scatterplot Saturation\nScatterplots can become saturated when too many points overlap in a small area of the chart, making it difficult to distinguish dense regions from sparse ones. For example, when plotting financial variables like production budget versus worldwide gross:\n\n\nCode\nsaturated = alt.Chart(movies_cleaned).mark_point().encode(\n    alt.X('Production_Budget'),\n    alt.Y('Worldwide_Gross')\n).properties(\n    title=\"Saturated Scatterplot: Budget vs Gross\"\n)\nsaturated\n\n\n\n\n\n\n\n\n\n\n2.3.3 Using Binned Heatmap to Reduce Saturation\nTo address saturation, we can bin both variables and use a heatmap where the color intensity represents the number of movies that fall into each rectangular region of the grid. This makes dense areas more interpretable\n\n\nCode\nheatmap_scatter = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('Production_Budget', bin=alt.Bin(maxbins=60)),\n    alt.Y('Worldwide_Gross', bin=alt.Bin(maxbins=60)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    title=\"Binned Heatmap: Budget vs Gross\"\n)\nheatmap_scatter\n\n\n\n\n\n\n\n\n\n\n2.3.4 Side-by-side Comparison\nCompare the raw scatterplot with the heatmap representation:\n\n\nCode\nsaturated | heatmap_scatter"
  },
  {
    "objectID": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "href": "eda.html#bivariate-analysis-categorical-vs-categorical",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.4 Bivariate Analysis: Categorical vs Categorical",
    "text": "2.4 Bivariate Analysis: Categorical vs Categorical\nWhen working with two categorical variables, bivariate analysis helps us understand how categories from one variable relate or are distributed across the other. For example, we might want to know how different movie genres are rated according to the MPAA rating system. Visualization techniques like grouped bar charts and faceted plots can reveal patterns, associations, or class imbalances.\n\n2.4.1 Basic Faceted Bar Chart\nWe begin by exploring how movies are rated (MPAA_Rating) across different genres (Major_Genre). A faceted bar chart allows us to visualize this relationship by plotting a bar chart per genre, helping to identify genre-specific rating distributions.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre'\n)\n\n\n\n\n\n\n\n\n\n\n2.4.2 Vertical Faceting for Alignment\nFaceting horizontally can make comparisons across genres harder when the x-axis is misaligned. By specifying columns=1, we lay out the facets vertically, making it easier to compare counts across genres.\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=1\n)\n\n\n\n\n\n\n\n\n\n\n2.4.3 Dependent vs Independent Axis Scaling\nBy default, facet plots share the same x-axis scale (dependent scale), which allows for easier comparison across panels. However, when the number of observations varies greatly between genres, this shared scale can compress some charts.\nWe can instead use independent x-axis scaling for each facet. This highlights the relative distribution within each genre.\n\n\nCode\nshared_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n)\n\nindependent_scale = alt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X('count()'),\n    alt.Y('MPAA_Rating'),\n    alt.Color('MPAA_Rating')\n).facet(\n    'Major_Genre',\n    columns=4\n).resolve_scale(x='independent')\n\nshared_scale | independent_scale\n\n\n\n\n\n\n\n\nThe left panel (shared scale) makes absolute comparisons between genres, while the right panel (independent scale) makes within-genre comparisons more readable.\n\n\n2.4.4 Heatmaps\nHeatmaps are effective for visualizing the relationship between two categorical variables when the goal is to display counts or frequency of occurrences. They map the number of observations to color, providing an intuitive view of which category pairs are most or least common.\nWe can enhance this basic representation by also using marker size, combining both color intensity and circle area to represent counts more effectively. This dual encoding can improve interpretation, especially when printed in grayscale or when there are subtle color differences.\n\n\nCode\nheatmap_color = alt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()')\n).properties(\n    title=\"Heatmap with Color (Count of Movies)\"\n)\n\nheatmap_size = alt.Chart(movies_cleaned).mark_circle().encode(\n    alt.X('MPAA_Rating'),\n    alt.Y('Major_Genre', sort='color'),\n    alt.Color('count()'),\n    alt.Size('count()')\n).properties(\n    title=\"Heatmap with Color + Size (Count of Movies)\"\n)\n\nheatmap_color | heatmap_size"
  },
  {
    "objectID": "eda.html#multivariate-analysis",
    "href": "eda.html#multivariate-analysis",
    "title": "Exploratory Data Analysis (EDA)",
    "section": "2.5 Multivariate Analysis",
    "text": "2.5 Multivariate Analysis\nMultivariate analysis helps us understand the interactions and relationships among multiple variables simultaneously. In the context of numerical features, it is useful to explore pairwise distributions, correlations, and detect potential clusters or anomalies.\nWhen the number of variables is large, repeated charts such as histograms or scatter plot matrices help us summarize patterns efficiently and consistently across all numerical dimensions.\n\n2.5.1 Repeated Histograms for Numerical Columns\nWe first identify and isolate all numerical columns from the dataset. Then we repeat a histogram for each of these columns to understand the individual distributions. This overview is helpful to detect skewness, outliers, or binning decisions that affect how data is grouped visually.\n\n\nCode\n# Select only numerical columns\nnumerical_columns = movies_cleaned.select_dtypes('number').columns.tolist()\n\n\n\n\nCode\nalt.Chart(movies_cleaned).mark_bar().encode(\n    alt.X(alt.repeat(),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y('count()')\n).properties(\n    width=150,\n    height=150\n).repeat(\n    numerical_columns,\n    columns=4\n)\n\n\n\n\n\n\n\n\n\n\n2.5.2 Scatter Plot Matrix (Pairplot)\nA scatter plot matrix shows the pairwise relationships between all numerical variables. This is a common exploratory tool to detect:\n\nCorrelations between variables\nOutliers or clusters\nRelationships useful for prediction models (e.g., to predict rating or budget)\n\nWe focus especially on the plots below the diagonal, as they are not duplicated.\n\n\nCode\nalt.Chart(movies_cleaned).mark_point().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='quantitative'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Heatmap Matrix\nWhen scatter plots become too saturated (many overlapping points), heatmaps offer a better alternative by binning the numeric values and encoding the count in color intensity.\n\n\nCode\nalt.Chart(movies_cleaned).mark_rect().encode(\n    alt.X(alt.repeat('column'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Y(alt.repeat('row'),type='quantitative',bin=alt.Bin(maxbins=30)),\n    alt.Color('count()'),\n    alt.Tooltip('count()')\n).properties(\n    width=100,\n    height=100\n).repeat(\n    column=numerical_columns,\n    row=numerical_columns\n).resolve_scale(\n    color='independent'\n)\n\n\n\n\n\n\n\n\nTo gain deeper insights into the dataset, it‚Äôs important to analyze how numerical variables behave across different categories. This type of multivariate analysis allows us to:\n\nCompare distributions across categories\nDetect outliers within categories\nObserve central tendency (median, quartiles) and spread (range, IQR)\n\nBoxplots are particularly effective for this purpose. In the following visualizations, we explore these relationships by repeating plots across combinations of categorical and numerical features.\n\n\n2.5.4 Filter Categorical Columns\nFirst, we select the relevant categorical columns, excluding identifiers and text-heavy variables like movie titles or director names.\n\n\nCode\ncategorical_columns =  movies_cleaned.select_dtypes('object').columns.to_list()\n\ncategorical_columns_remove = ['Title','Release_Date','Distributor','Director']\n\ncategorical_filtered = [col for col in categorical_columns if col not in categorical_columns_remove]\n\n\n\n\n2.5.5 Repeated Boxplots: Categorical vs Numerical\nWe repeat boxplots using combinations of categorical (rows) and numerical (columns) features. This matrix layout gives a clear visual overview of how numerical values are distributed within each category.\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X(alt.repeat('column'),type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    column=numerical_columns,\n    row=categorical_filtered\n)\n\n\n\n\n\n\n\n\n\n\n2.5.6 Faceted Boxplots\nFor more focused analysis, we can facet the boxplots using a specific categorical variable like MPAA_Rating, and repeat the chart by different categorical rows. This lets us keep the numerical axis fixed while comparing how categories vary across different classes (e.g., movie ratings).\n\n\nCode\nalt.Chart(movies_cleaned).mark_boxplot().encode(\n    alt.X('Running_Time_min', type='quantitative'),\n    alt.Y(alt.repeat('row'),type='nominal'),\n    alt.Size('count()'),\n    alt.Tooltip('Title:N')\n).properties(\n    width=100,\n    height=100\n).facet(\n    column='MPAA_Rating'\n).repeat(\n    row=categorical_filtered\n)"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Classification",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n\n\n\nCode\ndata = load_breast_cancer()\ndata\n\n\n{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]], shape=(569, 30)),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer Wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\n\nCode\n# data\nX = data.data #features\ny = data.target #etiqueta\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=42,stratify=y\n)\n\n# print(\"X_TRAIN\")\n# print(X_train)\n# print(\"X_TEST\")\n# print(X_test)\n\n# print(\"Y TRAIN\")\n# print(y_train)\n# print(\"Y_TEST\")\n# print(y_test)\n\n\n\n\n\n\n\nCode\nmodel = LogisticRegression(max_iter=100000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=100000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n100000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\nCode\ny_pred = model.predict(X_test)\n\n\n\n\n\n\n\nCode\naccuracy = accuracy_score(y_test,y_pred)\nprecision = precision_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\nf1 = f1_score(y_test,y_pred)\n\nprint(f\"accuracy: {accuracy}\")\nprint(f\"precision: {precision}\")\nprint(f\"recall: {recall}\")\nprint(f\"f1: {f1}\")\n\n\naccuracy: 0.9649122807017544\nprecision: 0.9594594594594594\nrecall: 0.9861111111111112\nf1: 0.9726027397260274\n\n\n\n\n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
  },
  {
    "objectID": "metrics.html#cargar-el-dataset-1",
    "href": "metrics.html#cargar-el-dataset-1",
    "title": "Classification",
    "section": "2. Cargar el Dataset",
    "text": "2. Cargar el Dataset\n\n\nCode\ndata = load_breast_cancer()\ndata\n\n\n{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]], shape=(569, 30)),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer Wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\n\nCode\n# data\nX = data.data #features\ny = data.target #etiqueta"
  },
  {
    "objectID": "metrics.html#pipeline",
    "href": "metrics.html#pipeline",
    "title": "Classification",
    "section": "Pipeline",
    "text": "Pipeline\n\n\nCode\npipe = Pipeline([\n    (\"escalado\",StandardScaler()),\n    (\"logreg\",LogisticRegression(max_iter=100000))\n])"
  },
  {
    "objectID": "metrics.html#train",
    "href": "metrics.html#train",
    "title": "Classification",
    "section": "Train",
    "text": "Train\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LogisticRegression(max_iter=100000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n100000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "metrics.html#test-o-predicci√≥n",
    "href": "metrics.html#test-o-predicci√≥n",
    "title": "Classification",
    "section": "Test o predicci√≥n",
    "text": "Test o predicci√≥n\n\n\nCode\ny_pred_pipe = pipe.predict(X_test)"
  },
  {
    "objectID": "metrics.html#evaluaci√≥n-1",
    "href": "metrics.html#evaluaci√≥n-1",
    "title": "Classification",
    "section": "Evaluaci√≥n",
    "text": "Evaluaci√≥n\n\n\nCode\nacc = accuracy_score(y_test,y_pred_pipe)\npre = precision_score(y_test,y_pred_pipe)\nre = recall_score(y_test,y_pred_pipe)\nfs = f1_score(y_test,y_pred_pipe)\n\nprint(acc)\nprint(pre)\nprint(re)\nprint(fs)\n\n\n0.9824561403508771\n0.9861111111111112\n0.9861111111111112\n0.9861111111111112"
  },
  {
    "objectID": "metrics.html#confusi√≥n-matrix",
    "href": "metrics.html#confusi√≥n-matrix",
    "title": "Classification",
    "section": "Confusi√≥n Matrix",
    "text": "Confusi√≥n Matrix\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred_pipe )"
  },
  {
    "objectID": "metrics.html#import-libraries",
    "href": "metrics.html#import-libraries",
    "title": "Classification",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "metrics.html#cargar-el-dataset",
    "href": "metrics.html#cargar-el-dataset",
    "title": "Classification",
    "section": "",
    "text": "Code\ndata = load_breast_cancer()\ndata\n\n\n{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]], shape=(569, 30)),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer Wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 569\\n\\n:Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n:Attribute Information:\\n    - radius (mean of distances from center to points on the perimeter)\\n    - texture (standard deviation of gray-scale values)\\n    - perimeter\\n    - area\\n    - smoothness (local variation in radius lengths)\\n    - compactness (perimeter^2 / area - 1.0)\\n    - concavity (severity of concave portions of the contour)\\n    - concave points (number of concave portions of the contour)\\n    - symmetry\\n    - fractal dimension (\"coastline approximation\" - 1)\\n\\n    The mean, standard error, and \"worst\" or largest (mean of the three\\n    worst/largest values) of these features were computed for each image,\\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n    10 is Radius SE, field 20 is Worst Radius.\\n\\n    - class:\\n            - WDBC-Malignant\\n            - WDBC-Benign\\n\\n:Summary Statistics:\\n\\n===================================== ====== ======\\n                                        Min    Max\\n===================================== ====== ======\\nradius (mean):                        6.981  28.11\\ntexture (mean):                       9.71   39.28\\nperimeter (mean):                     43.79  188.5\\narea (mean):                          143.5  2501.0\\nsmoothness (mean):                    0.053  0.163\\ncompactness (mean):                   0.019  0.345\\nconcavity (mean):                     0.0    0.427\\nconcave points (mean):                0.0    0.201\\nsymmetry (mean):                      0.106  0.304\\nfractal dimension (mean):             0.05   0.097\\nradius (standard error):              0.112  2.873\\ntexture (standard error):             0.36   4.885\\nperimeter (standard error):           0.757  21.98\\narea (standard error):                6.802  542.2\\nsmoothness (standard error):          0.002  0.031\\ncompactness (standard error):         0.002  0.135\\nconcavity (standard error):           0.0    0.396\\nconcave points (standard error):      0.0    0.053\\nsymmetry (standard error):            0.008  0.079\\nfractal dimension (standard error):   0.001  0.03\\nradius (worst):                       7.93   36.04\\ntexture (worst):                      12.02  49.54\\nperimeter (worst):                    50.41  251.2\\narea (worst):                         185.2  4254.0\\nsmoothness (worst):                   0.071  0.223\\ncompactness (worst):                  0.027  1.058\\nconcavity (worst):                    0.0    1.252\\nconcave points (worst):               0.0    0.291\\nsymmetry (worst):                     0.156  0.664\\nfractal dimension (worst):            0.055  0.208\\n===================================== ====== ======\\n\\n:Missing Attribute Values: None\\n\\n:Class Distribution: 212 - Malignant, 357 - Benign\\n\\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n:Donor: Nick Street\\n\\n:Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. dropdown:: References\\n\\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n    San Jose, CA, 1993.\\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\\n    July-August 1995.\\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n    163-171.\\n',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\n\nCode\n# data\nX = data.data #features\ny = data.target #etiqueta"
  },
  {
    "objectID": "metrics.html#split-data",
    "href": "metrics.html#split-data",
    "title": "Classification",
    "section": "",
    "text": "Code\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=42,stratify=y\n)\n\n# print(\"X_TRAIN\")\n# print(X_train)\n# print(\"X_TEST\")\n# print(X_test)\n\n# print(\"Y TRAIN\")\n# print(y_train)\n# print(\"Y_TEST\")\n# print(y_test)"
  },
  {
    "objectID": "metrics.html#entrenar-el-model",
    "href": "metrics.html#entrenar-el-model",
    "title": "Classification",
    "section": "",
    "text": "Code\nmodel = LogisticRegression(max_iter=100000)\nmodel.fit(X_train,y_train)\n\n\nLogisticRegression(max_iter=100000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n100000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "metrics.html#predict",
    "href": "metrics.html#predict",
    "title": "Classification",
    "section": "",
    "text": "Code\ny_pred = model.predict(X_test)"
  },
  {
    "objectID": "metrics.html#evaluacion",
    "href": "metrics.html#evaluacion",
    "title": "Classification",
    "section": "",
    "text": "Code\naccuracy = accuracy_score(y_test,y_pred)\nprecision = precision_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\nf1 = f1_score(y_test,y_pred)\n\nprint(f\"accuracy: {accuracy}\")\nprint(f\"precision: {precision}\")\nprint(f\"recall: {recall}\")\nprint(f\"f1: {f1}\")\n\n\naccuracy: 0.9649122807017544\nprecision: 0.9594594594594594\nrecall: 0.9861111111111112\nf1: 0.9726027397260274"
  },
  {
    "objectID": "metrics.html#matriz-de-confusi√≥n",
    "href": "metrics.html#matriz-de-confusi√≥n",
    "title": "Classification",
    "section": "",
    "text": "Code\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
  },
  {
    "objectID": "metrics.html#split-data-1",
    "href": "metrics.html#split-data-1",
    "title": "Classification",
    "section": "3. Split data",
    "text": "3. Split data\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=42,stratify=y\n)"
  },
  {
    "objectID": "metrics.html#train-modelo",
    "href": "metrics.html#train-modelo",
    "title": "Classification",
    "section": "Train modelo",
    "text": "Train modelo\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LogisticRegression(max_iter=100000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n100000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "metrics.html#test-o-prediction",
    "href": "metrics.html#test-o-prediction",
    "title": "Classification",
    "section": "Test o prediction",
    "text": "Test o prediction\n\n\nCode\ny_pred_pipe = pipe.predict(X_test)"
  },
  {
    "objectID": "metrics.html#evaluacion-1",
    "href": "metrics.html#evaluacion-1",
    "title": "Classification",
    "section": "6. Evaluacion",
    "text": "6. Evaluacion\n\n\nCode\naccuracy = accuracy_score(y_test,y_pred_pipe)\nprecision = precision_score(y_test,y_pred_pipe)\nrecall = recall_score(y_test,y_pred_pipe)\nf1 = f1_score(y_test,y_pred_pipe)\n\nprint(f\"accuracy: {accuracy}\")\nprint(f\"precision: {precision}\")\nprint(f\"recall: {recall}\")\nprint(f\"f1: {f1}\")\n\n\naccuracy: 0.9824561403508771\nprecision: 0.9861111111111112\nrecall: 0.9861111111111112\nf1: 0.9861111111111112"
  },
  {
    "objectID": "metrics.html#matriz-de-confusi√≥n-1",
    "href": "metrics.html#matriz-de-confusi√≥n-1",
    "title": "Classification",
    "section": "Matriz de confusi√≥n",
    "text": "Matriz de confusi√≥n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred_pipe)"
  },
  {
    "objectID": "MIDTER.html",
    "href": "MIDTER.html",
    "title": "MIDTERM",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error, r2_score"
  },
  {
    "objectID": "MIDTER.html#problema-regresion",
    "href": "MIDTER.html#problema-regresion",
    "title": "MIDTERM",
    "section": "PROBLEMA REGRESION",
    "text": "PROBLEMA REGRESION\nPredicci√≥n Termocuple_min(24) en datos de eficiencia energetca de vivienda"
  },
  {
    "objectID": "MIDTER.html#problema-calisficacion",
    "href": "MIDTER.html#problema-calisficacion",
    "title": "LAB1. PERSONAS DESAPARECIDAS EN ECUADOR",
    "section": "PROBLEMA CALISFICACION",
    "text": "PROBLEMA CALISFICACION\nclasificar una persona como ‚Äúempleado‚Äù vs ‚Äúdesempleado‚Äù bas√°ndote en variables num√©ricas como edad, horas trabajadas, nivel de educaci√≥n num√©rico codificado, ingreso, etc."
  },
  {
    "objectID": "MIDTER.html#import-libraries",
    "href": "MIDTER.html#import-libraries",
    "title": "MIDTERM",
    "section": "",
    "text": "Code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error, r2_score"
  },
  {
    "objectID": "MIDTER.html#variables-independientes-y-variable-objetivo",
    "href": "MIDTER.html#variables-independientes-y-variable-objetivo",
    "title": "MIDTERM",
    "section": "2. Variables independientes y variable objetivo",
    "text": "2. Variables independientes y variable objetivo\n\n\nCode\nX = data[['TempAir_max ¬∫C','HumAir_max ¬∫C','TempAir_min ¬∫C']]\ny = data[['HumAir_min ¬∫C']]\n\nprint(X)\nprint(y)\n\n\n     TempAir_max ¬∫C  HumAir_max ¬∫C  TempAir_min ¬∫C\n0             22.43           92.1           15.90\n1             23.85           93.0           14.45\n2             24.16           92.3           14.86\n3             22.63           93.1           14.58\n4             24.82           92.8           14.69\n..              ...            ...             ...\n199           24.06           92.1           13.82\n200           23.18           91.9           14.45\n201           22.00           92.5           14.04\n202           22.18           93.4           12.77\n203           23.51           92.5           13.66\n\n[204 rows x 3 columns]\n     HumAir_min ¬∫C\n0            68.90\n1            51.35\n2            55.23\n3            65.61\n4            51.53\n..             ...\n199          48.51\n200          55.52\n201          68.68\n202          52.13\n203          50.88\n\n[204 rows x 1 columns]"
  },
  {
    "objectID": "MIDTER.html#problema-clasificacion",
    "href": "MIDTER.html#problema-clasificacion",
    "title": "MIDTERM",
    "section": "PROBLEMA CLASIFICACION",
    "text": "PROBLEMA CLASIFICACION\nPredecir el sector publico/privado de un dataset de pago de bono de desempleo"
  },
  {
    "objectID": "MIDTER.html#carga-de-datos",
    "href": "MIDTER.html#carga-de-datos",
    "title": "MIDTERM",
    "section": "1. Carga de datos",
    "text": "1. Carga de datos\n\n\nCode\ndata = pd.read_excel('data regresion.xlsx')\ndata\n\n\n\n\n\n\n\n\n\nFecha - date\nTempAir_Avg\\n¬∫C\nHumAir_Avg %\nTermocuple_Avg(24) ¬∫C\nTempAir_max ¬∫C\nHumAir_max ¬∫C\nTermocuple_max(24) ¬∫C\nTempAir_min ¬∫C\nHumAir_min ¬∫C\nTermocuple_min(24)\n\n\n\n\n0\n2022-05-11\n18.131765\n85.298676\n27.136471\n22.43\n92.1\n27.91\n15.90\n68.90\n25.88\n\n\n1\n2022-05-12\n19.732662\n70.696883\n24.767987\n23.85\n93.0\n26.87\n14.45\n51.35\n19.86\n\n\n2\n2022-05-13\n19.923765\n73.838519\n24.948519\n24.16\n92.3\n27.43\n14.86\n55.23\n17.80\n\n\n3\n2022-05-14\n18.581579\n82.738947\n24.538355\n22.63\n93.1\n26.40\n14.58\n65.61\n19.45\n\n\n4\n2022-05-15\n20.484815\n75.320000\n25.934012\n24.82\n92.8\n28.84\n14.69\n51.53\n18.65\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n199\n2022-11-26\n19.632914\n69.829735\n25.949073\n24.06\n92.1\n28.87\n13.82\n48.51\n18.21\n\n\n200\n2022-11-27\n19.131071\n73.641643\n25.606643\n23.18\n91.9\n28.40\n14.45\n55.52\n18.73\n\n\n201\n2022-11-28\n17.565274\n83.274863\n24.372534\n22.00\n92.5\n26.40\n14.04\n68.68\n18.68\n\n\n202\n2022-11-29\n18.733377\n73.794834\n24.994305\n22.18\n93.4\n28.08\n12.77\n52.13\n17.41\n\n\n203\n2022-11-30\n20.230169\n64.898814\n22.590847\n23.51\n92.5\n25.36\n13.66\n50.88\n18.56\n\n\n\n\n204 rows √ó 10 columns"
  },
  {
    "objectID": "MIDTER.html#split-data",
    "href": "MIDTER.html#split-data",
    "title": "MIDTERM",
    "section": "3. Split data",
    "text": "3. Split data\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=42\n)"
  },
  {
    "objectID": "MIDTER.html#pipeline",
    "href": "MIDTER.html#pipeline",
    "title": "MIDTERM",
    "section": "4. Pipeline",
    "text": "4. Pipeline\n\n\nCode\npipe = Pipeline([\n    (\"escalado\",StandardScaler()),\n    (\"logreg\",LinearRegression())\n])"
  },
  {
    "objectID": "MIDTER.html#train-modelo",
    "href": "MIDTER.html#train-modelo",
    "title": "MIDTERM",
    "section": "5. Train modelo",
    "text": "5. Train modelo\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nPipeline(steps=[('escalado', StandardScaler()), ('logreg', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept¬†\nTrue\n\n\n\ncopy_X¬†\nTrue\n\n\n\ntol¬†\n1e-06\n\n\n\nn_jobs¬†\nNone\n\n\n\npositive¬†\nFalse"
  },
  {
    "objectID": "MIDTER.html#test-o-prediction",
    "href": "MIDTER.html#test-o-prediction",
    "title": "MIDTERM",
    "section": "6. Test o prediction",
    "text": "6. Test o prediction\n\n\nCode\ny_pred_pipe = pipe.predict(X_test)"
  },
  {
    "objectID": "MIDTER.html#evaluacion",
    "href": "MIDTER.html#evaluacion",
    "title": "MIDTERM",
    "section": "7. Evaluacion",
    "text": "7. Evaluacion\n\n\nCode\n# Evaluar el modelo\nmse = mean_squared_error(y_test, y_pred_pipe)\nr2 = r2_score(y_test, y_pred_pipe)\nprint(\"MSE:\", mse)\nprint(\"R2:\", r2)\n\n\nMSE: 28.83933122330356\nR2: 0.7729267679935119"
  },
  {
    "objectID": "MIDTER.html#valores-reales-vs-predichos",
    "href": "MIDTER.html#valores-reales-vs-predichos",
    "title": "MIDTERM",
    "section": "8. Valores reales vs predichos",
    "text": "8. Valores reales vs predichos\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.scatter(y_test, y_pred_pipe)\nplt.xlabel(\"Valores Reales (y_test)\")\nplt.ylabel(\"Valores Predichos (y_pred_pipe)\")\nplt.title(\"Gr√°fico de Dispersi√≥n: Valores Reales vs Predichos\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.show()"
  },
  {
    "objectID": "MIDTER.html#curva-de-aprendizaje",
    "href": "MIDTER.html#curva-de-aprendizaje",
    "title": "MIDTERM",
    "section": "9. Curva de aprendizaje",
    "text": "9. Curva de aprendizaje\nfrom sklearn.model_selection import learning_curve train_sizes, train_scores, test_scores = learning_curve( pipe, X, y, cv=5, n_jobs=-1, train_sizes=[0.1, 0.33, 0.55, 0.78, 1.0], scoring=‚Äòr2‚Äô ) import numpy as np train_scores_mean = np.mean(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) plt.plot(train_sizes, train_scores_mean, ‚Äòo-‚Äô, color=‚Äúr‚Äù, label=‚ÄúTraining score‚Äù) plt.plot(train_sizes, test_scores_mean, ‚Äòo-‚Äô, color=‚Äúg‚Äù, label=‚ÄúCross-validation score‚Äù) plt.xlabel(‚ÄúTraining examples‚Äù) plt.ylabel(‚ÄúScore‚Äù) plt.title(‚ÄúLearning Curve‚Äù) plt.legend(loc=‚Äúbest‚Äù) plt.show()"
  },
  {
    "objectID": "MIDTER.html#cargar-data",
    "href": "MIDTER.html#cargar-data",
    "title": "MIDTERM",
    "section": "1. Cargar Data",
    "text": "1. Cargar Data\n\n\nCode\ndata = pd.read_excel('farmacias_con_clusters.xlsx')\ndata = data.dropna()\ndata\n\n\n\n\n\n\n\n\n\nfarmacia\ntipofarmacia\nventa\ntrx\nnomina\nsucursal\ncluster\nperfil\n\n\n\n\n0\nECO QUITO SAN JOSE DE MINAS\nFRANQUICIA\n29341.6987\n2991\n2\nECONOMICAS\n0\n44\n\n\n1\nECO MODERNA\nFRANQUICIA\n61003.9857\n5700\n4\nECONOMICAS\n0\n46\n\n\n2\nECO OTAVALO 31 DE OCTUBRE\nFRANQUICIA\n37030.7980\n5984\n3\nECONOMICAS\n0\n46\n\n\n3\nECO OTAVALO COPACABANA\nFRANQUICIA\n47367.4135\n6742\n4\nECONOMICAS\n0\n47\n\n\n4\nECO CRUZ ROJA OTAVALO\nFRANQUICIA\n63026.8760\n6949\n4\nECONOMICAS\n0\n48\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n958\nECO QUITO CIUDAD FUTURA\nFRANQUICIA\n32064.8565\n4928\n2\nECONOMICAS\n99\n51\n\n\n959\nECO CHILLOGALLO\nFRANQUICIA\n25195.3154\n3007\n2\nECONOMICAS\n99\n52\n\n\n960\nECO QUITO LUCHA MEDIA\nFRANQUICIA\n16635.9057\n1856\n1\nECONOMICAS\n99\n52\n\n\n961\nECO QUITO TAMBILLO\nFRANQUICIA\n47009.0518\n5961\n3\nECONOMICAS\n99\n52\n\n\n962\nECO QUITUMBE REINA DEL CISNE\nFRANQUICIA\n35663.2024\n2976\n2\nECONOMICAS\n99\n52\n\n\n\n\n963 rows √ó 8 columns"
  },
  {
    "objectID": "MIDTER.html#variables-independientes-y-variable-objetivo-1",
    "href": "MIDTER.html#variables-independientes-y-variable-objetivo-1",
    "title": "MIDTERM",
    "section": "2. Variables independientes y variable objetivo",
    "text": "2. Variables independientes y variable objetivo\n\n\nCode\nX = data[['venta','trx']]\ny = data[['sucursal']]\n\ny['sucursal'] = y['sucursal'].replace({'ECONOMICAS': 0, 'MEDICITY': 1})\n\n\nprint(X)\nprint(y)\n\n\n          venta   trx\n0    29341.6987  2991\n1    61003.9857  5700\n2    37030.7980  5984\n3    47367.4135  6742\n4    63026.8760  6949\n..          ...   ...\n958  32064.8565  4928\n959  25195.3154  3007\n960  16635.9057  1856\n961  47009.0518  5961\n962  35663.2024  2976\n\n[963 rows x 2 columns]\n     sucursal\n0           0\n1           0\n2           0\n3           0\n4           0\n..        ...\n958         0\n959         0\n960         0\n961         0\n962         0\n\n[963 rows x 1 columns]\n\n\nC:\\Users\\Lenovo i5\\AppData\\Local\\Temp\\ipykernel_27424\\2158973334.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  y['sucursal'] = y['sucursal'].replace({'ECONOMICAS': 0, 'MEDICITY': 1})\nC:\\Users\\Lenovo i5\\AppData\\Local\\Temp\\ipykernel_27424\\2158973334.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y['sucursal'] = y['sucursal'].replace({'ECONOMICAS': 0, 'MEDICITY': 1})"
  },
  {
    "objectID": "MIDTER.html#split-data-1",
    "href": "MIDTER.html#split-data-1",
    "title": "MIDTERM",
    "section": "3. Split data",
    "text": "3. Split data\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,test_size=0.2,random_state=40,stratify=y\n)"
  },
  {
    "objectID": "MIDTER.html#pipeline-1",
    "href": "MIDTER.html#pipeline-1",
    "title": "MIDTERM",
    "section": "4. Pipeline",
    "text": "4. Pipeline\n\n\nCode\npipe = Pipeline([\n    (\"escalado\",StandardScaler()),\n    (\"logreg\",LogisticRegression(max_iter=100000))\n])"
  },
  {
    "objectID": "MIDTER.html#train-modelo-1",
    "href": "MIDTER.html#train-modelo-1",
    "title": "MIDTERM",
    "section": "5. Train modelo",
    "text": "5. Train modelo\n\n\nCode\npipe.fit(X_train,y_train)\n\n\nC:\\Users\\Lenovo i5\\Documents\\MAESTRIA IA YACHAY TECH\\APRENDIZAJE DE MAQUINA\\SEMANA 1\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nPipeline(steps=[('escalado', StandardScaler()),\n                ('logreg', LogisticRegression(max_iter=100000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps¬†\n[('escalado', ...), ('logreg', ...)]\n\n\n\ntransform_input¬†\nNone\n\n\n\nmemory¬†\nNone\n\n\n\nverbose¬†\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy¬†\nTrue\n\n\n\nwith_mean¬†\nTrue\n\n\n\nwith_std¬†\nTrue\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\npenalty¬†\n'l2'\n\n\n\ndual¬†\nFalse\n\n\n\ntol¬†\n0.0001\n\n\n\nC¬†\n1.0\n\n\n\nfit_intercept¬†\nTrue\n\n\n\nintercept_scaling¬†\n1\n\n\n\nclass_weight¬†\nNone\n\n\n\nrandom_state¬†\nNone\n\n\n\nsolver¬†\n'lbfgs'\n\n\n\nmax_iter¬†\n100000\n\n\n\nmulti_class¬†\n'deprecated'\n\n\n\nverbose¬†\n0\n\n\n\nwarm_start¬†\nFalse\n\n\n\nn_jobs¬†\nNone\n\n\n\nl1_ratio¬†\nNone"
  },
  {
    "objectID": "MIDTER.html#predict",
    "href": "MIDTER.html#predict",
    "title": "MIDTERM",
    "section": "6. Predict",
    "text": "6. Predict\n\n\nCode\ny_pred = pipe.predict(X_test)"
  },
  {
    "objectID": "MIDTER.html#evaluacion-1",
    "href": "MIDTER.html#evaluacion-1",
    "title": "MIDTERM",
    "section": "7. Evaluacion",
    "text": "7. Evaluacion\n\n\nCode\naccuracy = accuracy_score(y_test,y_pred)\nprecision = precision_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\nf1 = f1_score(y_test,y_pred)\n\nprint(f\"accuracy: {accuracy}\")\nprint(f\"precision: {precision}\")\nprint(f\"recall: {recall}\")\nprint(f\"f1: {f1}\")\n\n\naccuracy: 0.8808290155440415\nprecision: 0.8333333333333334\nrecall: 0.5128205128205128\nf1: 0.6349206349206349"
  },
  {
    "objectID": "MIDTER.html#matriz-de-confusi√≥n",
    "href": "MIDTER.html#matriz-de-confusi√≥n",
    "title": "MIDTERM",
    "section": "8. Matriz de Confusi√≥n",
    "text": "8. Matriz de Confusi√≥n\n\n\nCode\nConfusionMatrixDisplay.from_predictions(y_test,y_pred)"
  },
  {
    "objectID": "MIDTER.html#curva-roc",
    "href": "MIDTER.html#curva-roc",
    "title": "MIDTERM",
    "section": "9. Curva ROC",
    "text": "9. Curva ROC\n\n\nCode\nfrom sklearn.metrics import RocCurveDisplay \nRocCurveDisplay.from_predictions(y_test, y_pred)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay     \nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Precision:\", precision_score(y_test, y_pred))\nprint(\"Recall:\", recall_score(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\n\n\nAccuracy: 0.8808290155440415\nPrecision: 0.8333333333333334\nRecall: 0.5128205128205128\nF1 Score: 0.6349206349206349"
  }
]